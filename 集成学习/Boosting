Boosting:
首先，Boosting是一个迭代提升的过程，所以它肯定是串行的算法（尽管xgboost可以在节点分裂属性选择上做并行计算）。
基于训练集，先训练弱学习器，然后根据前一个弱学习器分错的样本，改变样本的概率分布构成新的训练集，从而可以训练
出一个更强的学习器。这样反复迭代提升，就能得到一系列分类器。最后，将这些分类器组合起来，就能构成一个很强的学习器。


Adaboost是一种弱学习算法到强学习算法，这里的弱和强学习算法，指的当然都是分类器，首先我们需要简单介绍几个概念。

1：弱学习器：在二分情况下弱分类器的错误率会低于50%。其实任意的分类器都可以做为弱分类器，比如之前介绍的KNN、决策树、Naïve Bayes、logiostic回归和SVM都可以。这里我们采用的弱分类器是单层决策树，它是一个单节点的决策树。它是adaboost中最流行的弱分类器，当然并非唯一可用的弱分类器。即从特征中选择一个特征来进行分类，该特征能使错误率达到最低，注意这里的错误率是加权错误率，为错分样本(1)与该样本的权重的乘积之和(不明白看后面代码)。

更为严格点的定义：
强学习：一个概念如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么，这个概念是强可学习的；
弱学习：一个概念如果存在一个多项式的学习算法能够学习它，并且学习的正确率仅比随机猜测略好(高于50%)，那么，这个概念是弱可学习的；
强可学习与弱可学习是等价的。并且理论证明可以将若干个弱学习分类器通过线性叠加提升为强学习分类器。

2：强学习器：识别准确率很高并能在多项式时间内完成的学习算法

AdaBoost的主要思想就是在不改变训练数据的情况下，通过在迭代训练弱学习器中，不断提升被错分类样本的权重（也就是使被错分的样本在下一轮训练时得到更多的重视）
不断减少正确分类样本的权重。最后通过加权线性组合M个弱分类器得到最终的分类器，正确率越高的弱分类器的投票权数越高，正确率低的弱分类器自然投票权数就低。

GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的
结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。GBDT中的树是回归树（不是分类树），GBDT用来做回归预测，调整后也可以用于
分类。GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。

