Boosting:
首先，Boosting是一个迭代提升的过程，所以它肯定是串行的算法（尽管xgboost可以在节点分裂属性选择上做并行计算）。
基于训练集，先训练弱学习器，然后根据前一个弱学习器分错的样本，改变样本的概率分布构成新的训练集，从而可以训练
出一个更强的学习器。这样反复迭代提升，就能得到一系列分类器。最后，将这些分类器组合起来，就能构成一个很强的学习器。


Adaboost是一种弱学习算法到强学习算法，这里的弱和强学习算法，指的当然都是分类器，首先我们需要简单介绍几个概念。

1：弱学习器：在二分情况下弱分类器的错误率会低于50%。其实任意的分类器都可以做为弱分类器，比如之前介绍的KNN、决策树、Naïve Bayes、logiostic回归和SVM都
可以。这里我们采用的弱分类器是单层决策树，它是一个单节点的决策树。它是adaboost中最流行的弱分类器，当然并非唯一可用的弱分类器。即从特征中选择一个特征来
进行分类，该特征能使错误率达到最低，注意这里的错误率是加权错误率，为错分样本(1)与该样本的权重的乘积之和(不明白看后面代码)。

更为严格点的定义：
强学习：一个概念如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么，这个概念是强可学习的；
弱学习：一个概念如果存在一个多项式的学习算法能够学习它，并且学习的正确率仅比随机猜测略好(高于50%)，那么，这个概念是弱可学习的；
强可学习与弱可学习是等价的。并且理论证明可以将若干个弱学习分类器通过线性叠加提升为强学习分类器。

2：强学习器：识别准确率很高并能在多项式时间内完成的学习算法

AdaBoost的主要思想就是在不改变训练数据的情况下，通过在迭代训练弱学习器中，不断提升被错分类样本的权重（也就是使被错分的样本在下一轮训练时得到更多的重视）
不断减少正确分类样本的权重。最后通过加权线性组合M个弱分类器得到最终的分类器，正确率越高的弱分类器的投票权数越高，正确率低的弱分类器自然投票权数就低。

GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的
结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。GBDT中的树是回归树（不是分类树），GBDT用来做回归预测，调整后也可以用于
分类。GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。

这里我们再看看GBDT分类算法
GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。
为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，
我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。


xgboost

与gbdt相比，具体的优点有：
1.损失函数是用泰勒展式二项逼近，而不是像gbdt里就是一阶导数
2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性
3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的

xgboost与gdbt除了上述三点的不同，xgboost在实现时还做了许多优化：

1、在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个
可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。
2、xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。
3、xgboost借鉴了随机森林中的列（特征）采样技术，即在某个节点分裂时，不是在当前节点中所有属性中选取最佳分裂属性，而是在当前属性集合中的某些属性
中来选择最优分裂属性。这种方法降低了过拟合的可能性。
4、特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。

5、按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，
可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。

6、xgboost还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

